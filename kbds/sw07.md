# SW07 - NLP 4 (F)

üéØ **Lernziel**: Sie kennen den Unterschied zwischen Word2Vec mit Skip Grams und CBOW.



üéØ **Lernziel**: Den Einsatz von Sliding Windows und Bag of Words k√∂nnen sie mit einem Beispiel erl√§utern.



## MEP (Tonspur)

2 Ans√§tze:

* (C)BOW (schlechtere Performance da kein Kontext, daf√ºr schneller)

* Skip Gram (bessere Performance, daf√ºr aufw√§ndiger)

Bag of Words vs. TF-IDF

Sparse Matrix (Vektor mit vielen 0en und wenigen 1en)

Domain: Oft schwierig, Texte mit Fachwissen zu findein

Modelle kontrollieren (Dimensionen reduzieren zB mit PCA), plotten und pr√ºfen.



## Bag of Words

Bag of Words bringt jedoch auch einige Probleme mit sich:

* Ber√ºcksichtigt den Kontext nicht, die Ordnung der W√∂rter ist nicht relevant => Verliert Semantik / Kontext
* N-Grams
* Sparse Vectors (ben√∂tigt Rechenleistung)



Bag of Words kann einfach mit `CountVectorizer()` von Sklearn implementiert werden. F√ºr jedes Wort im Vokabular wird die H√§ufigkeit gez√§hlt.

## Continuous Bag of Words (CBOW)

Erweitert BoW, sodass der Kontext der W√∂rter miteinbezogen wird. Dazu nutzt CBOW Sliding Windows (*Context Window*) um den Kontext zu erfassen. Dabei unterscheiden sich Skip-Gram und CBOW lediglich darin, welchen Input sie nehmen und welchen Output sie erzeugen:

* **Skip-Gram** erzeugt aus einem Target Word (Input) $\rightarrow$  mehrere Context W√∂rter (Output)

* **CBOW** erzeugt aus mehreren Context W√∂rter (Input) $\rightarrow$ ein Target Word (Output)



Skip-Grams wird die Cosine Similarity immer einzeln zwicshen einem Target Word $t_i$ und dem Context Word $w_i$ berechnet:

![image-20201103185548658](assets/image-20201103185548658.png)

CBOW wiederrum berechnet die Cosine Similarity zwischen einem Durchschnittsvektor $W_D$ (= Average Embedding) der Context Words und dem Target Word $t_i$.

![image-20201103185644387](assets/image-20201103185644387.png)

### Average Embedding

Folgendes Beispiel: *Flag* ist das Target Word $t_i$, die gelben und gr√ºnen W√∂rter sind die Context Words (*the* = $w_{i-2}$, *swiss* = $w_{i-1}$, *is* = $w_{i+1}$, *red* = $w_{i+1}$)

![image-20201103185728640](assets/image-20201103185728640.png)

??



## CBOW vs Skip-Gram

**Skip-Gram**:

* Gut geeignet auch f√ºr kleine Datens√§tze
* Kann auch seltene W√∂rter mit tiefer H√§ufigkeit akkurat erfassen

**CBOW**:

* Ist schneller beim Training (Berechnung) mit grossen Datens√§tzen
* Kann gut mit h√§ufigen W√∂rtern umgehen



Folgende Tabelle zeigt diese Unterscheide besonders gut:

![image-20201103190255971](assets/image-20201103190255971.png)

W√§hrned Skip-Grams eine bessere (semantische) Accuracy erzielten, ben√∂tigte es daf√ºr jedoch viel mehr Trainingszeit.



## Knowledge Base

Als Beispiel soll eine Wissensdatenbank f√ºr Aus- und Weiterbildungen erstellt werden. Die Frage ist also, wie kommt man vom Bild (links) zum Resultat (rechts)?

![image-20201103193148324](assets/image-20201103193148324.png)

**Input (Daten)**

* Lebensl√§ufe
* Jobbeschreibungen (z.B. von Jobportalen)
* Aus- und Weiterbildungen (z.B. mittels Beschreibungen der Kurse)
* usw.

**Vorgehen**

* Daten werden aggregiert (z.B. standardisiert und zentral gespeichert)
* Tokenization, Stop Word Removal, Word2Vec
* (Bei kleinen Datens√§tzen: Word2Vec mit Skip-Gram)

**Output**

* Word Embedding (*n*-dimensionale Repr√§sentation der W√∂rter)
* W√∂rter sind nach ihrer semantischen √Ñhnlichkeit (Kontext) verteilt

Um dieses Ziel zu erreichen, k√∂nnen aus einer **zweidimensionalen Wortrepr√§sentation** die entsprechenden **Konzepte** mitsamt ihrem **√Ñnhlichkeitsmass** in eine Graphdatenbank transformiert werden.

## Dimensionality Reduction & PCA

> üéØ **Lernziel**:  Sie kennen dein Einsatzzweck von PCA und k√∂nnen grob beschreiben wie die Dimensionsreduktion ausgef√ºhrt wird.

Das Problem: Aus einer *n*-dimensionalen Repr√§sentation der W√∂rter soll auf zwei Dimensionen hinuntergebrochen werden:

![image-20201103191215036](assets/image-20201103191215036.png)

Dabei k√∂nnen die bestehenden Distanzen (respektive Winkel) zwischen den Vektoren im *n*-dimensionalen Raum als Distanzen der Punkten in dem zweidimensionalen Raum verwendet werden.

Die Herausforderung dabei liegt darin, w√§hrend der Reduktion diese bestehenden Distanzen so gut wie m√∂glich zu bewahren.

Die Principal Component Analysis (PCA) hilft bei dieser Reduktion. Dieses mathematische Verfahren reduziert diese Repr√§sentation mit m√∂glichst wenig Informationsverlust reduziert werden. Dabei verwendet es eine Art von Komprimierung (Data Compression), welche sich auf Distanzen fokussiert welche **m√∂glichst stark variieren**. 

Im folgenden Beispiel hat die Dimension *PLZ* keine Varianz und kann somit ohne grossen Verlust komprimiert werden.

![image-20201103191807215](assets/image-20201103191807215.png)

Dabei soll die Varianz der Datenpunkte auf einer neuen Achse (der *Principal Component*) maximiert werden, also eine maximale Streuung erreicht werden. Diese Achse entspricht einem **Eigenvektor**. Die Summe der Distanzen zwischen dem Eigenvektor und den Datenpunkten (den **Eigenwerten**) soll wiederum minimiert werden.

![image-20201103192030564](assets/image-20201103192030564.png)



## √úberf√ºhrung in Graphdatenbanken

>  üéØ **Lernziel**: Sie kennen m√∂gliche Ans√§tze zur √úberf√ºhrung der W√∂rter aus der 2D ‚ÄúWortmappe‚Äú in eine Graph Datenbank.



Liegt die Repr√§sentation in einem zweidimensionalen Raum vor, k√∂nnen die W√∂rter und deren Beziehungen einfach in eine Graphdatenbank √ºberf√ºhrt werden. Dabei sind die W√∂rter die Knoten in einem Graphen und die Kanten haben als Wert die Distanz aus der 2D Repr√§sentation. Dieser Wert sollte normiert sein und steht daf√ºr, wie **√§hnlich** sich diese W√∂rter sind.

![image-20201103192143604](assets/image-20201103192143604.png)

Somit k√∂nnen einfach √§hnliche W√∂rter gefunden werden:

![image-20201103192308606](assets/image-20201103192308606.png)



### Wortkombinationen

Nicht alle Kombinationen von √§hnlichen W√∂rter machen auch Sinn (z.B. *Marketing HR*). Solche Wortkombinationen k√∂nnen herausgefiltert werden, indem einzelne W√∂rter kombiniert werden und danach die Distanz zu einem weiteren Wort berechnet wird, zum Beispiel wie gut passt *Sales* zu *Marketing Leiter*?

![image-20201103192535986](assets/image-20201103192535986.png)



## Konzept-Extraktion

> üéØ **Lernziel**: Probleme und L√∂sungsans√§tze zur Konzept-Extraktion sind ihnen gel√§ufig.

Konzepte sind f√ºr eine dom√§nenspezifische Wissensdatenbank (respektive f√ºr die Modellierung einer solchen DB) relevante W√∂rter oder Wortkombinationen. So ist jedes Konzept auch ein Wort, umgekehrt ist nicht jedes Wort ein Konzept.

Welche W√∂rter als Konzepte gelten sollen ist schwierig. Oftmals existiert keine Referenzdatenbank f√ºr eine spezifische Anwendung / spezifische Dom√§ne.

Dazu existieren einige L√∂sungsans√§tze:

* Referenzdatenbank (sofern vorhanden)
* Manuelle Klassifikation (sog. Concept Whitelist)
* Dom√§nenspezifische & statistische Verfahren
* Maneulle Klassifikation durch Benutzende (z.B. Experten)

![image-20201103193024401](assets/image-20201103193024401.png)





## Testat

1. Erstellen Sie f√ºr das GOT Model eine 2D Visualisierung von 50 Begriffen.
2. F√ºr Game of Thrones soll eine spezifische Wissensdatenbank zu den genutzten Waffen erstellt werden. Wie w√ºrden sie die Konzeptextraktion vornehmen? (Vorgehen kurz erl√§utern)