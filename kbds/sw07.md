# SW07 - NLP 4 (F)



## MEP (Tonspur)

Beschreibung der zwei Ans√§tzen:

* (C)BoW: Tiefere Qualit√§t da der Kontext nicht miteinbezogen wird, daf√ºr deutlich schneller.
* Skip Grams: H√∂here Qualit√§t da Kontext, daf√ºr aufw√§ndiger / rechenintensiver.

**Bag of Words vs. TF-IDF**

Bag of Words z√§hlt lediglich die W√∂rter (H√§ufigkeit) w√§hrend TF-IDF die Relevanz extrahiert.

**Sparse Matrix / Vektor**

Ein Vektor, welcher nur wenige Stellen ungleich 0 hat. Beispielsweise ein Vektor mit 30 Dimensionen, hat aber nur an genau 2 Stellen keine 0.

**NLP - Herausforderungen**

* Es ist oft schwierig, viel (Text-)Daten zu einer bestimmten zu finden (Texte mit Fachwissen)

* Modelle immer kontrollieren, z.B. mittels Dimensionality Reduction (PCA) und dann plotten & pr√ºfen

## Bag of Words

>  üéØ **Lernziel**: Den Einsatz von Sliding Windows und Bag of Words k√∂nnen sie mit einem Beispiel erl√§utern.

Bag of Words (BOW) ist ein Verfahren mit dem W√∂rter aus Dokumenten in einem Model repr√§sentiert werden. Das Model dient als Basis f√ºr den Einsatz von Machine Learning.

Das Model setzt voraus, dass das bestehende Vokabular (e.g. alle W√∂rter in allen Dokumenten eines Korpusses) voraus sowie ein Messverfahren, mit welchem die H√§ufigkeit gemessen werden kann.

Als Beispiel seien folgende drei S√§tze gegeben:

1. *Das Wetter ist sch√∂n und es ist sonnig*
2. *Sonniges Wetter ist sch√∂n*
3. *Sobald das Wetter sch√∂n ist gehen wir schwimmen*

Das Vokabular baut sich aus allen unique W√∂rtern zusammen, hier: *{das, wetter, ist, sch√∂n, und, es, sonnig, sonniges, sobald, gehen, wir, schwimmen}* (abgek√ºrzt)

Im n√§chsten Schritt werden die S√§tze in Vektoren √ºberf√ºhrt: Jedes Wort im Vokabular stellt eine Dimension dar. Dabei werden lediglich die H√§ufigkeit eines Wortes im Satz gez√§hlt (Vokabular sortiert)

|        | das  | es   | gehen | ist  | schwimmen | sch√∂n | sobald | und  | wetter | wir  |
| ------ | ---- | ---- | ----- | ---- | --------- | ----- | ------ | ---- | ------ | ---- |
| Satz 1 | 1    | 1    | 0     | 2    | 0         | 1     | 0      | 1    | 1      | 0    |
| Satz 2 | 0    | 0    | 0     | 1    | 0         | 1     | 0      | 0    | 1      | 0    |
| Satz 3 | 1    | 0    | 1     | 1    | 1         | 1     | 1      | 0    | 1      | 1    |

Als Vektoren:

 ![image-20210117095447079](assets/image-20210117095447079.png)



Bag of Words bringt jedoch auch einige Probleme mit sich:

* Verliert Semantik / Kontext der W√∂rter: Ber√ºcksichtigt den Kontext nicht, die Ordnung der W√∂rter ist nicht relevant 
* N-Grams
* Bei einem grossen Vokabular entstehen Sparse Vektoren welche folglich zu Sparse Matrix f√ºhren. Diese *Sparsness* macht den Umgang mit solchen Vektoren ineffizient (ben√∂tigt Rechenleistung). Zum Beispiel: das Vokabular umfasst 500 W√∂rter, die meisten S√§tze / Dokumente im Korpus haben jedoch nur 10 W√∂rter.
* Transformation von Sparse Vektoren in einen Dense Vektor kann mit folgenden Technoken vorgenommen werden: Stemming, Stop Word Removal, N-Gram Detection und Einsatz von Features.

Bag of Words kann einfach mit `CountVectorizer()` von Sklearn implementiert werden. F√ºr jedes Wort im Vokabular wird die H√§ufigkeit gez√§hlt.

## Continuous Bag of Words (CBOW)

Erweitert BoW, sodass der Kontext der W√∂rter miteinbezogen wird. Dazu nutzt CBOW Sliding Windows (*Context Window*) um den Kontext zu erfassen. Dabei unterscheiden sich Skip-Gram und CBOW lediglich darin, welchen Input sie nehmen und welchen Output sie erzeugen:

* **Skip-Gram** erzeugt aus einem Target Word (Input) $\rightarrow$  mehrere Context W√∂rter (Output)

* **CBOW** erzeugt aus mehreren Context W√∂rter (Input) $\rightarrow$ ein Target Word (Output)



Skip-Grams wird die Cosine Similarity immer einzeln zwicshen einem Target Word $t_i$ und dem Context Word $w_i$ berechnet:

![image-20201103185548658](assets/image-20201103185548658.png)

CBOW wiederrum berechnet die Cosine Similarity zwischen einem Durchschnittsvektor $W_D$ (= Average Embedding) der Context Words und dem Target Word $t_i$.

![image-20201103185644387](assets/image-20201103185644387.png)

### Average Embedding

Folgendes Beispiel: *Flag* ist das Target Word $t_i$, die gelben und gr√ºnen W√∂rter sind die Context Words (*the* = $w_{i-2}$, *swiss* = $w_{i-1}$, *is* = $w_{i+1}$, *red* = $w_{i+1}$)

![image-20201103185728640](assets/image-20201103185728640.png)



## CBOW vs Skip-Gram

>  üéØ **Lernziel**: Sie kennen den Unterschied zwischen Word2Vec mit Skip Grams und CBOW.

**Skip-Gram**:

* Gut geeignet auch f√ºr kleine Datens√§tze
* Kann auch seltene W√∂rter mit tiefer H√§ufigkeit akkurat erfassen

**CBOW**:

* Ist schneller beim Training (Berechnung) mit grossen Datens√§tzen
* Kann gut mit h√§ufigen W√∂rtern umgehen



Folgende Tabelle zeigt diese Unterscheide besonders gut:

![image-20201103190255971](assets/image-20201103190255971.png)

W√§hrned Skip-Grams eine bessere (semantische) Accuracy erzielten, ben√∂tigte es daf√ºr jedoch viel mehr Trainingszeit.



## Knowledge Base

Als Beispiel soll eine Wissensdatenbank f√ºr Aus- und Weiterbildungen erstellt werden. Die Frage ist also, wie kommt man vom Bild (links) zum Resultat (rechts)?

![image-20201103193148324](assets/image-20201103193148324.png)

**Input (Daten)**

* Lebensl√§ufe
* Jobbeschreibungen (z.B. von Jobportalen)
* Aus- und Weiterbildungen (z.B. mittels Beschreibungen der Kurse)
* usw.

**Vorgehen**

* Daten werden aggregiert (z.B. standardisiert und zentral gespeichert)
* Tokenization, Stop Word Removal, Word2Vec
* (Bei kleinen Datens√§tzen: Word2Vec mit Skip-Gram)

**Output**

* Word Embedding (*n*-dimensionale Repr√§sentation der W√∂rter)
* W√∂rter sind nach ihrer semantischen √Ñhnlichkeit (Kontext) verteilt

Um dieses Ziel zu erreichen, k√∂nnen aus einer **zweidimensionalen Wortrepr√§sentation** die entsprechenden **Konzepte** mitsamt ihrem **√Ñnhlichkeitsmass** in eine Graphdatenbank transformiert werden.

## Dimensionality Reduction & PCA

> üéØ **Lernziel**:  Sie kennen dein Einsatzzweck von PCA und k√∂nnen grob beschreiben wie die Dimensionsreduktion ausgef√ºhrt wird.

Das Problem: Aus einer *n*-dimensionalen Repr√§sentation der W√∂rter soll auf zwei Dimensionen hinuntergebrochen werden:

![image-20201103191215036](assets/image-20201103191215036.png)

Dabei k√∂nnen die bestehenden Distanzen (respektive Winkel) zwischen den Vektoren im *n*-dimensionalen Raum als Distanzen der Punkten in dem zweidimensionalen Raum verwendet werden.

Die Herausforderung dabei liegt darin, w√§hrend der Reduktion diese bestehenden Distanzen so gut wie m√∂glich zu bewahren.

Die Principal Component Analysis (PCA) hilft bei dieser Reduktion. Dieses mathematische Verfahren reduziert diese Repr√§sentation mit m√∂glichst wenig Informationsverlust reduziert werden. Dabei verwendet es eine Art von Komprimierung (Data Compression), welche sich auf Distanzen fokussiert welche **m√∂glichst stark variieren**. 

Im folgenden Beispiel hat die Dimension *PLZ* keine Varianz und kann somit ohne grossen Verlust komprimiert werden.

![image-20201103191807215](assets/image-20201103191807215.png)

Dabei soll die Varianz der Datenpunkte auf einer neuen Achse (der *Principal Component*) maximiert werden, also eine maximale Streuung erreicht werden. Diese Achse entspricht einem **Eigenvektor**. Die Summe der Distanzen zwischen dem Eigenvektor und den Datenpunkten (den **Eigenwerten**) soll wiederum minimiert werden.

![image-20201103192030564](assets/image-20201103192030564.png)



## √úberf√ºhrung in Graphdatenbanken

>  üéØ **Lernziel**: Sie kennen m√∂gliche Ans√§tze zur √úberf√ºhrung der W√∂rter aus der 2D ‚ÄúWortmappe‚Äú in eine Graph Datenbank.

Liegt die Repr√§sentation in einem zweidimensionalen Raum vor, k√∂nnen die W√∂rter und deren Beziehungen einfach in eine Graphdatenbank √ºberf√ºhrt werden. Dabei sind die W√∂rter die Knoten in einem Graphen und die Kanten haben als Wert die Distanz aus der 2D Repr√§sentation. Dieser Wert sollte normiert sein und steht daf√ºr, wie **√§hnlich** sich diese W√∂rter sind.

![image-20201103192143604](assets/image-20201103192143604.png)

Somit k√∂nnen einfach √§hnliche W√∂rter gefunden werden:

![image-20201103192308606](assets/image-20201103192308606.png)



### Wortkombinationen

Nicht alle Kombinationen von √§hnlichen W√∂rter machen auch Sinn (z.B. *Marketing HR*). Solche Wortkombinationen k√∂nnen herausgefiltert werden, indem einzelne W√∂rter kombiniert werden und danach die Distanz zu einem weiteren Wort berechnet wird, zum Beispiel wie gut passt *Sales* zu *Marketing Leiter*?

![image-20201103192535986](assets/image-20201103192535986.png)



## Konzept-Extraktion

> üéØ **Lernziel**: Probleme und L√∂sungsans√§tze zur Konzept-Extraktion sind ihnen gel√§ufig.

Konzepte sind f√ºr eine dom√§nenspezifische Wissensdatenbank (respektive f√ºr die Modellierung einer solchen DB) relevante W√∂rter oder Wortkombinationen. So ist jedes Konzept auch ein Wort, umgekehrt ist nicht jedes Wort ein Konzept.

Welche W√∂rter als Konzepte gelten sollen ist schwierig. Oftmals existiert keine Referenzdatenbank f√ºr eine spezifische Anwendung / spezifische Dom√§ne.

Dazu existieren einige L√∂sungsans√§tze:

* Referenzdatenbank (sofern vorhanden)
* Manuelle Klassifikation (sog. Concept Whitelist)
* Dom√§nenspezifische & statistische Verfahren
* Maneulle Klassifikation durch Benutzende (z.B. Experten)

![image-20201103193024401](assets/image-20201103193024401.png)


