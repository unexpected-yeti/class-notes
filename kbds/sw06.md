# Natural Language Processing â€“ Basics 3

## Word2Vec

> ğŸ¯ **Lernziel**: Sie kÃ¶nnen die Funktionsweise von Word2Vec mittels Skip-Grams erlÃ¤utern

Word2Vec ist ein Kontext- und Vorhersage (Prediction)-basierter Ansatz zur Ermittlung der Verwandtschaft zwischen WÃ¶rtern.

![image-20210117101940195](assets/image-20210117101940195.png)

Word2Vec verwendet ein trainiertes, neuronales Netzwerk um die Ã„hnlichkeit zwischen einem Input Word und irgendeinem Wort im Vokabular anzugeben. Die Ã„hnlichkeit wird mit einem Wert zwischen 0 und 1 als Cosine Similarity ausgedrÃ¼ckt.

Als Beispiel: Wie Ã¤hnlich sind sich *Bear* und *Animal*?D

Word2Vec kann zwei Methoden zur Messung des Verwandtschaftsgrades verwenden:

**1. Skip-Gram Model**

Nimmt als Input ein Wort $w_t$ und gibt als Output den Kontext des Wortes zurÃ¼ck.

**2. CBOW Model**

Wiederum nimmt als Input den Kontext eines Wortes und gibt das Wort zurÃ¼ck.



Folgendes Bild zeigt die Unterschiede dieser beiden Methoden im Kontext von Word2Vec:

![image-20210117100128800](assets/image-20210117100128800.png)



![image-20210117102154917](assets/image-20210117102154917.png)



## Skip-Grams

> ğŸ¯ **Lernziel**: Sie kÃ¶nnen das Vorgehen beim Einsatz von Skip-Grams mit einem Beispiel erklÃ¤ren. 

Skip-Grams ziehen die Semantik (Kontext) eines Wortes mitein. Dabei zieht es nicht nur das nÃ¤chste Wort mitein, sondern auch WÃ¶rter in einem bestimmten Abstand zum Wort hinzu (es *skippt* WÃ¶rter dazwischen). 

Dieses Vorgehen berÃ¼cksichtigt mehr Kombinationen und hat daher eine hÃ¶here Wahrscheinlichkeit, akkuratere Kombinationen zu identifizieren. 

**Beispiel**

In Blau ist das Target Word (= Input Word)

![image-20210117100549755](assets/image-20210117100549755.png)

Ein weiteres Beispiel:

![image-20210117100800365](assets/image-20210117100800365.png)





## One Hot Vector

>  ğŸ¯ **Lernziel**: Transformation und Einsatz von One-Hot Vectors sind ihnen gelÃ¤ufig.

Vorgehen um WÃ¶rter in One Hot Vectors zu transformieren:

1. Bestimmen des Vokabulars anhand der einzigartigen (discint) WÃ¶rter
2. WÃ¶rter z.B. alphabetisch in absteigender Ordnung in einer Liste erfassen
3. Jedes Wort hat innerhalb der Liste eine eindeutige Position (*ID*)
4. FÃ¼r jedes Wort den entsprechenden (1xV) Vektor erstellen

![image-20210117101237621](assets/image-20210117101237621.png)

Problem: Sie haben oft eine sehr hohe DimensionalitÃ¤t!

### One Hot Encoding

Als One Hot Encoding wird die resultierende Abbildung der jeweiligen WÃ¶rter im Sliding Window (Target word = Orange, Bi-Gram & Skip-Gram = GrÃ¼n) bezeichnet:

![image-20210117101634525](assets/image-20210117101634525.png)

## Features

Um das Problem mit den hohen Dimensionen anzugehen kÃ¶nnen dedizierte **Features** eingesetzt werden. Dabei handelt es sich um thematische Gruppierungen, welche im Kontext des Vokabulars einen mÃ¶glichst hohen Grad an Abdeckung erreichen.

Der Zusammenhang der WÃ¶rter wird innerhalb der Features modelliert. Dadurch reduziert sich die KomplexitÃ¤t bei der Berechnung, zumal nicht mehr mit (1xV) Vektoren gerechnet werden muss

Eine Kennzahl: Zwischen 100-300 Features werden angestrebt.

![image-20210117101812849](assets/image-20210117101812849.png)

Somit werden die WÃ¶rter in Konzepte (Features) zusammengefasst und in der Matrix die Wahrscheinlichkeit / Score dass ein Worter zu diesem Konzept gehÃ¶rt.



## Anwendung

> ğŸ¯ **Lernziel**: Sie sind in der Lage mit den erwÃ¤hnten Operatoren einen Datensatz zu bearbeiten.

Siehe Code-Samples in den Folien oder Testat.