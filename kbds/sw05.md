# Natural Language Processing ‚Äì Basics 2

## Word Embeddings

üéØ **Lernziel**: Sie kennen die unterschiedlichen Vorgehensweisen im Umgang mit Word Embeddings.

> Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.

Durch Word Embeddings wird der *Verwantschaftsgrad* einzelner W√∂rter.  Um diesen Grad zu bestimmten gibt es verschiedene Ans√§tze:

* Frequency-based:
  * N-Grams
  * TF-IDF
  * Co-Occurrence
* Prediction-based (Neural Networks & Vektoren):

  * Skip-Gram
  * Continuous Bag of Words (CBOW)

Beispiel: `New + <word> / <word> = York (92%), Jersey (87%), Orleans (78%)`



## N-Grams

üéØ **Lernziel**: Sie wissen wie N-Grams berechnet werden und k√∂nnen dies selbst√§ndig vornehmen.

>  **N-Gramme** sind das Ergebnis der Zerlegung eines Textes in Fragmente. Der Text wird dabei zerlegt, und jeweils $N$ aufeinanderfolgende Fragmente werden als *N-Gramm* zusammengefasst. 

Die Tokenization / Lemmatization listet alle W√∂rter einzeln. Bei Bezeichnungen, die aus Wortkombinationen bestehen (z.B. *New York* oder *Hochschule Luzern*) wird dies problematisch. Solche Kombinationen werden in NLP als N-Gram bezeichnet. Der Fokus liegt dabei darauf, **nachfolgende W√∂rter zu identifizieren**. $N$ definiert die Anzahl W√∂rter, welche eine Wortkombination ausmachen.

*Typische* N-Gramme sind:

* Unigram: Einzelne W√∂rter, beispielsweise nach Tokenization
* Bigram: 2 W√∂rter ($n = 2$), z.B. (New York), (Hochschule Luzern)
* Trigram: 3 W√∂rter ($n=3$), z.B. (The White House)
* Four-Gram: 4 W√∂rter ($n=4$), z.B. (The New York Times)
* etc.



### N-Gram-Analyse

Die Analyse von N-Grams versucht die Frage zu beantworten, wie wahrscheinlich auf eine bestimmte Wortfolge ein bestimmtes Wort folgt: $P(w|h)$

Dabei ist $P$ die Wahrscheinlichkeit (Probability), dass $w$ das n√§chste Wort einer Abfolge ist, gegeben die historischen Daten $h$. 

#### Beispiel:

Wie hoch ist die Wahrscheinlichkeit, dass *York* nach der Eingabe von *I love New* erscheint? Hinweis: $C$ steht f√ºr Korpus:
$$
P(\text{York}|\text{I love New}) = C(\text{I love new york} / C(\text{I love new}))
$$

Diese Wahrscheinlichkeit kann mit der H√§ufigkeit / Occurrance / Frequenz, mit welcher *New York* in einem Korpus erscheint berechnet werden.



## Vorgehensweise

N-Grams verhalten sich wie ein Sliding Window, welches √ºber den Text gelegt wird. Dabei entstehen √ºberlappende Tupel der L√§nge $n$. Folgend ein Beispiel f√ºr Bigramme ($n=2$).

![image-20201017112415229](assets/image-20201017112415229.png)

Somit kann man die Bezeichnung *New York* (eine Wortkombination bestehend aus zwei W√∂rter), finden. 

Aus obiger Analyse kann folgende Co-Occurrence Matrix erstellt werden:

![image-20201017163658140](assets/image-20201017163658140.png)

### Daten bereinigen

Um N-Gramme sinnvoll einsetzen zu k√∂nnen, muss der Datensatz zuerst bereinigt werden. Ansonsten k√∂nnen die W√∂rter nicht sauber analysiert werden, zum Beispiel:

![image-20201017112931361](assets/image-20201017112931361.png)

In diesem Beispiel wurde die Bezeichnung *New York* mehrmals aufgelistet, da im zweiten N-Gram noch Satzzeichen enthalten sind. Diese interessieren uns nicht und k√∂nnen bereinigt werden. Uns interessieren nur die W√∂rter respektive die Wortkombinationen.

Ein Beispiel mit NLTK um Bi- und Trigrams zu bilden:

![carbon (3)](assets/carbon (3).png)

### Korpus

Der Korpus ist eine Sammlung von Texten (oder Audio). Die Gr√∂sse der Korpus ist dabei wichtig, da sie direkten Einfluss auf die "Treffsicherheit" des Algorithmus hat. Oftmals werden jedoch nur kleine Texte analysiert. Deshalb existieren auch Referenzkorpusse als Basis f√ºr die Identifikation, wie z.B.: https://www.ngrams.info/samples_coca1.asp

![image-20201017113217844](assets/image-20201017113217844.png)

### Stop Words entfernen

N-Grams beinhalten oft auch sehr uninteressante Wortkombinationen wie (it is), (of the), (in the). Diese k√∂nnen entfernt werden, z.B. mittels `stopwords` aus `nltk.corpus` (siehe Code Snippet oben).

![image-20201017115346540](assets/image-20201017115346540.png)



![image-20201017115415829](assets/image-20201017115415829.png)



### Summary

* Die N-Gram Detection ist einfach zu implementieren

* Sie sind stark abh√§ngig vom Korpus, welcher √ºblicherweise sehr klein ist. Es k√∂nnen Referenzkorpusse zur Hilfe gezogen werden.

* Identifikation von Bigrams ist schwierig, da die K√ºrze der Kombination das Resultat stark beinflusst.

* Daneben ist die Identifikation von Trigrams deutlicher genauer

* N-Gram Detektion ignoriert den Kontext, indem die W√∂rter sich befinden

* Wortkombinationen werden immer einzeln betrachtet, sprich es werden keine Annahmen auf Grund von bereits vorhandenen Wortkombinationen getroffen. Daher ist dieser Ansatz ineffizient.

  ![image-20201017115602132](assets/image-20201017115602132.png)



## TF-IDF 

üéØ **Lernziel**: Sie sind in der Lage mittels TF-IDF die Relevanz einzelner Terme, sowie von N-grams in den jeweiligen Dokumenten zu berechnen.

TF-IDF steht f√ºr die Term Frequency - Inverse Document Frequency. Es ist ein Mass f√ºr die Relevanz von Termen in einzelnen Dokumenten ($n$) einer Sammlung von Dokumenten ($N$). Sie ber√ºcksichtigt Worth√§ufigkeit in einem Korpus (mehrere Dokumente). Gleichzeitigt bestraft sie h√§ufige W√∂rter (z.B. der, die, das) und reduziert deren Relevanz mittels einer Gewichtung.

### Term Frequency (TF)

Term Frequency (TF) = $\text{tf}_{t,d}$ =  H√§ufigkeit Term $t$ in einem Dokument $d$. Umso h√∂her die TF, desto "wichtiger"  ist der Term f√ºr die Charakterisierung des Dokuments

#### Beispiel

![image-20201017115940782](assets/image-20201017115940782.png)

### Inverse Document Frequency (IDF)

Inverse Document Frequency (IDF) wiederum misst, in wie vielen Dokumenten $n$ aller Dokumenten $N$ der jeweilige Term vorkommt. Die Terme, die in einer Vielzahl an Dokumenten vorkommen sollen an Relevanz verlieren (Inverse = log())

#### Beispiel:

Das Wort *Das* ist f√ºr die Charakterisierung des Dokumentes nicht interessant, es ist somit nicht *relevant*. Die Relevanz wird daher reduziert. *Rotkreuz* hingegen ist relevanter, um den *jeweiligen* Text (hier: d1 und d2) zu klassifizieren:

![image-20201017120435422](assets/image-20201017120435422.png)

> *Welche Worte kommen zwar h√§ufig ein einem einzelnen Dokument vor, sind jedoch, auf alle Dokumente die wir uns anschauen, relativ einzigartig? Oder welche Worte kommen in allen Dokumenten vor und sind damit wahrscheinlich uninteressanter?*

### TF-IDF:

TF-IDF ist folglich die Kombination respektive das Produkt aus der TF und IDF.

#### Beispiel:

![image-20201017120554077](assets/image-20201017120554077.png)



### Code

Beispiel mit [Sklearn TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):

![carbon (4)](assets/carbon (4).png)

## Testat
- [x] Identifiziert die Bi- und Trigrams aus 'shakespeare-macbeth.txt'. Dieses Buch ist Teil des Gutenberg Corpus. Der Datensatz muss zun√§chst bereinigt werden (Stop Words, Satzzeichen, etc.)
- [x] Berechnet den TF-IDF Wert er Bigrams aus den Dokumenten IDB_1.txt, IMDB_2.txt, IMDB_3.txt